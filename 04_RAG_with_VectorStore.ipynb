{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3FXj1S2FLpY"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/04-RAG_with_VectorStore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BGJ3fxhOk2V"
      },
      "source": [
        "## Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dzEkDis3i81x",
        "outputId": "3d8a8b02-c08d-4312-ba50-9cda2721127e"
      },
      "outputs": [],
      "source": [
        "# !pip install -q openai==1.107.0 chromadb==1.0.21 google-genai==1.36.0 llama-index==0.14.0 llama-index-llms-google-genai==0.3.0 langchain==0.3.27 \\\n",
        "#                 llama-index-vector-stores-chroma==0.5.2 langchain-chroma==0.2.5 langchain-openai==0.3.32 langchain-google-genai==2.0.10 \\\n",
        "#                 jedi==0.19.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # take environment variables from .env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9JbAzFcjkpn"
      },
      "source": [
        "# Load the Dataset (CSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tif8-JoRH68"
      },
      "source": [
        "## Download\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fQaa1LN1mXL"
      },
      "source": [
        "The dataset includes several articles from the TowardsAI blog, which provide an in-depth explanation of the LLaMA2 model. Read the dataset as a long string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QTUkdfJjY4N",
        "outputId": "db410a2e-4019-4530-ebc1-1c96b14cfb77"
      },
      "outputs": [],
      "source": [
        "# !curl -o ./mini-dataset.csv https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk-4alIxROo8"
      },
      "source": [
        "## Read File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CYwRT6R0o0I",
        "outputId": "887f2b42-5a94-4ada-b9e0-d7bc82ebe1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "171044\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "# Load the file as a JSON\n",
        "with open(\"./mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "\n",
        "    for idx, row in enumerate(csv_reader):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "        text += row[1]\n",
        "\n",
        "# The number of characters in the dataset.\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17g2RYOjmf2"
      },
      "source": [
        "# Chunking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STACTMUR1z9N",
        "outputId": "dbdf9809-aef6-4351-ed55-292809049cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "335\n"
          ]
        }
      ],
      "source": [
        "chunk_size = 512\n",
        "chunks = []\n",
        "\n",
        "# Split the long text into smaller manageable chunks of 512 characters.\n",
        "for i in range(0, len(text), chunk_size):\n",
        "    chunks.append(text[i : i + chunk_size])\n",
        "\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fOomeMGqu10"
      },
      "source": [
        "#Interface of Chroma with LlamaIndex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CtdsIUQ81_hT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\DROPBOX\\Документы\\Карзановы\\Андрей\\__CHALLENGES\\AI_Full_Stack_Developer\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "# Convert the chunks to Document objects so the LlamaIndex framework can process them.\n",
        "doc_chunks = [Document(text=chunk) for chunk in chunks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "335"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(doc_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "## Save on Chroma -> changed to Qdrant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mXi56KTXk2sp"
      },
      "outputs": [],
      "source": [
        "# import chromadb\n",
        "\n",
        "# # create client and a new collection\n",
        "# # chromadb.EphemeralClient saves data in-memory.\n",
        "# chroma_client = chromadb.PersistentClient(path=\"./mini-chunked-dataset\")\n",
        "# chroma_collection = chroma_client.create_collection(\"mini-chunked-dataset\")\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# create client with persistent storage\n",
        "# qdrant_client = QdrantClient(path=\"./mini-chunked-dataset\")\n",
        "\n",
        "server_id = \"192.168.162.240\"\n",
        "# If you are running Qdrant in a Docker container on your local machine, use the\n",
        "\n",
        "qdrant_client = QdrantClient(url=f\"http://{server_id}:6333\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[CollectionDescription(name='Part3_DDofDS'),\n",
              " CollectionDescription(name='Part3_DDofDS_1_10'),\n",
              " CollectionDescription(name='scifact'),\n",
              " CollectionDescription(name='document_chat'),\n",
              " CollectionDescription(name='mini-chunked-dataset'),\n",
              " CollectionDescription(name='Part6-linkedin-posts'),\n",
              " CollectionDescription(name='Part3_DDofDS_SQuAD'),\n",
              " CollectionDescription(name='graphRAGstoreds'),\n",
              " CollectionDescription(name='Part4_Attention'),\n",
              " CollectionDescription(name='chat_with_docs'),\n",
              " CollectionDescription(name='graphRAG_FIXED_test'),\n",
              " CollectionDescription(name='Part3_DDofDS_SQuAD_qb')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qdrant_client.get_collections().collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection 'mini-chunked-dataset' already exists.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "collection_name = \"mini-chunked-dataset\"\n",
        "\n",
        "if not qdrant_client.collection_exists(collection_name):\n",
        "\n",
        "    print(f\"Creating collection '{collection_name}'...\")\n",
        "    # create a new collection\n",
        "    # You must specify vector size and distance metric\n",
        "    qdrant_client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=VectorParams(\n",
        "            # size=384,  # dimension of your embeddings (e.g., 384 for all-MiniLM-L6-v2)\n",
        "            size=1536,  # dimension of your embeddings (e.g., 1536 for text-embedding-3-small)\n",
        "            distance=Distance.COSINE,  # or Distance.EUCLID, Distance.DOT\n",
        "        ),\n",
        "    )\n",
        "else:\n",
        "    # qdrant_client.delete_collection(collection_name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !llama-index-vector-stores-qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jKXURvLtkuTS"
      },
      "outputs": [],
      "source": [
        "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "\n",
        "# Define a storage context object using the created vector database.\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"mini-chunked-dataset\")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "641a7cc706434941bc5791e3bab0eac7",
            "58da22841ed54907947a6aed1b6c9911",
            "68eb136ff6884b7e8c83cba04f0d1e07",
            "7e1e1bceaa9d4e8e97cc84ee43012d28",
            "0d6643b050244190bb3d01fe5c8bd88b",
            "79a4ea8d763c4a2bb47e4f4b5904cd0f",
            "91d0a9ea7766459b949a650f98029b79",
            "84a97db21c7943d492ef59d2772ddc06",
            "be291c6d702c46218926eaf5fd48d3b7",
            "0a0da58c54234ec19b1ca42de38a91b6",
            "138d656d6ae045e29df4a5666372aa8d",
            "643dd45f9b164ae687cf9c91533ad26b",
            "23a5a17062b04b918775fa4135823629",
            "45040e4fe7ea41e4a42e3b0886a9637d",
            "3919d4ff56bb46a9a0419735eb28880c",
            "2c945e23360345d192330312d191ceec",
            "c711f0f965744f5284175803fdc411a2",
            "4b9556cac2844546b137b722ceb5db6c",
            "027b9ed835be4c218cc804d9939ed140",
            "9ca4ca1e7e3b4928858929d25e6f1140",
            "9f54eb88444a48f3a96c17cc141befd5",
            "e999ad6863f842d6aa0b16d72af74fdd"
          ]
        },
        "id": "WsD52wtrlESi",
        "outputId": "c258c268-b6b7-48f3-f990-49b5386b49d2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "524d29edf65e49b984bef1a2734da659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/335 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6119dba114c443bb5e6914db9192cf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/335 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# Build index / generate embeddings using OpenAI embedding model\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    doc_chunks,\n",
        "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
        "    storage_context=storage_context,\n",
        "    show_progress=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JPD8yAinVSq"
      },
      "source": [
        "## Query Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install llama-index-llms-openrouter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzS13x1ZlZ5X"
      },
      "outputs": [],
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "\n",
        "# from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "# llm = GoogleGenAI(model=\"gemini-2.5-flash\",max_tokens=512, temperature=1)\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm_openai = OpenAI(model=\"gpt-4.1-mini\", max_tokens=512, temperature=1)\n",
        "query_engine_openai = index.as_query_engine(llm=llm_openai, similarity_top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYsQ4uLN_Oxg",
        "outputId": "9e930464-ad29-4cf9-ca86-ac77abc95de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLaMA 2 models are available in four different sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine_openai.query(\"How many parameters LLaMA2 model has?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.openrouter import OpenRouter\n",
        "\n",
        "# !pip install llama-index-llms-openrouter\n",
        "llm_openrouter = OpenRouter(\n",
        "    model=\"google/gemini-2.5-flash\",\n",
        "    max_tokens=512,\n",
        "    temperature=1,\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        ")\n",
        "\n",
        "query_engine_openrouter = index.as_query_engine(llm=llm_openrouter, similarity_top_k=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Llama 2 model comes in four different sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine_openrouter.query(\"How many parameters LLaMA2 model has?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWK571VNg-qR"
      },
      "source": [
        "# Interface of Chroma with LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMPAniL2e4NP"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.document import Document\n",
        "\n",
        "# Convert the chunks to Document objects so the LangChain framework can process them.\n",
        "documents = [Document(page_content=t) for t in chunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBt8qGxArUPD"
      },
      "source": [
        "## Save on Chroma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xas7HkuhJ8A"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Add the documents to chroma DB and create Index / embeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "chroma_db = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./mini-chunked-dataset\",\n",
        "    collection_name=\"mini-chunked-dataset\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8AXJJyBrZWF"
      },
      "source": [
        "## Query Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H64YLxshM2b"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initializing the LLM model\n",
        "#llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", max_tokens=512)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=512,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxBqPNtthPaa",
        "outputId": "1f552203-3163-4be0-cffd-ea4fadecbb5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "query = \"How many parameters LLaMA 2 model has?\"\n",
        "\n",
        "retriever = chroma_db.as_retriever(search_kwargs={\"k\": 4})\n",
        "# Define a RetrievalQA chain that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "response = chain.invoke(query)\n",
        "print(response[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKr16L_kwyYX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AI_Full_Stack_Developer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
